{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebcd1a14",
   "metadata": {},
   "source": [
    "## chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdaa186f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure connection: connected\n",
      "Embedding connection: connected\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from test_memory import get_llm, get_conversation_chain, get_memory, MODEL_CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b2475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the configured LLM\n",
    "llm = get_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a410319",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the capital of {country}?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5dfe117",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c28ef175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"country\": \"France\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08330748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but there is no recognized country or region named \"Adhiash.\" It's possible that you might be referring to a fictional place or there might be a spelling error. Could you provide more context or check the spelling?\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"country\": \"Adhiash\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "063071c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of the United States is Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"country\": \"USA\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb4c242",
   "metadata": {},
   "source": [
    "### sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89f134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "392cf3a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LLMChain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1: Generate ideas\u001b[39;00m\n\u001b[1;32m      2\u001b[0m ideas_template \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m      3\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      4\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerate 3 short ideas about: \u001b[39m\u001b[38;5;132;01m{topic}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m ideas_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39mideas_template)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Step 2: Evaluate ideas\u001b[39;00m\n\u001b[1;32m      9\u001b[0m evaluate_template \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m     10\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     11\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPick the best idea from: \u001b[39m\u001b[38;5;132;01m{text}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LLMChain' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Generate ideas\n",
    "ideas_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Generate 3 short ideas about: {topic}\"\n",
    ")\n",
    "ideas_chain = LLMChain(llm=llm, prompt=ideas_template)\n",
    "\n",
    "# Step 2: Evaluate ideas\n",
    "evaluate_template = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Pick the best idea from: {text}\"\n",
    ")\n",
    "evaluate_chain = LLMChain(llm=llm, prompt=evaluate_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6af145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequential chain\n",
    "chain = SimpleSequentialChain(\n",
    "    chains=[ideas_chain, evaluate_chain],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc59ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m1. **Community Solar Gardens**: Develop small-scale solar farms in local neighborhoods where residents can subscribe to receive clean energy directly from the solar garden. This initiative can democratize access to renewable energy, reduce electricity costs, and foster community engagement in sustainability efforts.\n",
      "\n",
      "2. **Floating Wind Turbines**: Invest in research and development of floating wind turbines that can be deployed in deep ocean waters. These turbines can harness stronger and more consistent winds found offshore, increasing energy production while minimizing land use and visual impact on coastal landscapes.\n",
      "\n",
      "3. **Renewable Energy Microgrids**: Implement microgrids powered by a combination of solar panels, wind turbines, and energy storage systems in remote or underserved areas. These microgrids can provide reliable and sustainable electricity, reduce dependency on fossil fuels, and enhance resilience against power outages and natural disasters.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mChoosing the best idea depends on various factors such as feasibility, impact, scalability, and specific goals. However, if we consider broad impact and potential for widespread implementation, **Community Solar Gardens** might be the best choice. Here's why:\n",
      "\n",
      "1. **Accessibility and Democratization**: Community Solar Gardens can make renewable energy accessible to people who might not have the means or space to install solar panels on their own property. This includes renters, low-income households, and those living in shaded areas.\n",
      "\n",
      "2. **Community Engagement**: This initiative fosters community involvement and awareness around sustainability, encouraging collective action and education about renewable energy benefits.\n",
      "\n",
      "3. **Cost Reduction**: By subscribing to a community solar garden, residents can potentially lower their electricity bills without the upfront costs of installing personal solar systems.\n",
      "\n",
      "4. **Scalability**: Community solar projects can be scaled to fit the needs of different neighborhoods and communities, making them adaptable to various geographic and demographic contexts.\n",
      "\n",
      "5. **Policy Support**: Many regions are increasingly supportive of community solar projects, with policies and incentives that can facilitate their development and expansion.\n",
      "\n",
      "While Floating Wind Turbines and Renewable Energy Microgrids are also excellent ideas with significant benefits, Community Solar Gardens offer a more immediate and accessible solution for urban and suburban areas, promoting widespread adoption of renewable energy.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Use the chain\n",
    "result = chain.run(\"renewable energy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b6faa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': None,\n",
       " 'memory': None,\n",
       " 'callbacks': None,\n",
       " 'verbose': True,\n",
       " 'tags': None,\n",
       " 'metadata': None,\n",
       " 'callback_manager': None,\n",
       " 'chains': [LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Generate 3 short ideas about: {topic}'), llm=CustomAzureChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fb90cc9a6b0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fb90cce10f0>, root_client=<openai.lib.azure.AzureOpenAI object at 0x7fb909368670>, root_async_client=<openai.lib.azure.AsyncAzureOpenAI object at 0x7fb90cc9a6e0>, model_name='DevGPT4o', temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'), top_p=0.9, disabled_params={'parallel_tool_calls': None}, azure_endpoint='https://azswcdevbktchatgpt.openai.azure.com/', deployment_name='DevGPT4o', openai_api_version='2024-02-01', openai_api_type='azure'), output_parser=StrOutputParser(), llm_kwargs={}),\n",
       "  LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='Pick the best idea from: {text}'), llm=CustomAzureChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fb90cc9a6b0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fb90cce10f0>, root_client=<openai.lib.azure.AzureOpenAI object at 0x7fb909368670>, root_async_client=<openai.lib.azure.AsyncAzureOpenAI object at 0x7fb90cc9a6e0>, model_name='DevGPT4o', temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'), top_p=0.9, disabled_params={'parallel_tool_calls': None}, azure_endpoint='https://azswcdevbktchatgpt.openai.azure.com/', deployment_name='DevGPT4o', openai_api_version='2024-02-01', openai_api_type='azure'), output_parser=StrOutputParser(), llm_kwargs={})],\n",
       " 'strip_outputs': False,\n",
       " 'input_key': 'input',\n",
       " 'output_key': 'output'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e63047",
   "metadata": {},
   "source": [
    "1. name: None\n",
    "Purpose: The name of the chain for identification\n",
    "Current Value: None (not set)\n",
    "Usage: Useful for debugging and logging when you have multiple chains\n",
    "2. memory: None\n",
    "Purpose: Memory component to store conversation history\n",
    "Current Value: None (no memory attached)\n",
    "Usage: Unlike ConversationChain, SimpleSequentialChain doesn't use memory by default\n",
    "3. callbacks: None\n",
    "Purpose: Callback functions for monitoring chain execution\n",
    "Current Value: None (no callbacks set)\n",
    "Usage: For logging, monitoring, or custom actions during chain execution\n",
    "4. verbose: True\n",
    "Purpose: Controls whether to show detailed output during execution\n",
    "Current Value: True (will show step-by-step execution)\n",
    "Usage: When True, you'll see each step's input/output\n",
    "5. tags: None\n",
    "Purpose: Tags for categorizing or filtering chains\n",
    "Current Value: None (no tags set)\n",
    "Usage: For organization and filtering in LangChain applications\n",
    "6. metadata: None\n",
    "Purpose: Additional metadata about the chain\n",
    "Current Value: None (no metadata set)\n",
    "Usage: For storing custom information about the chain\n",
    "7. callback_manager: None\n",
    "Purpose: Manages callback execution during chain runs\n",
    "Current Value: None (no callback manager)\n",
    "Usage: Coordinates multiple callbacks during execution\n",
    "8. chains: [LLMChain, LLMChain]\n",
    "Purpose: The actual chains that will be executed sequentially\n",
    "Current Value: Array with 2 LLMChain objects\n",
    "Usage: These are the steps that will run in order\n",
    "9. strip_outputs: False\n",
    "Purpose: Whether to strip whitespace from outputs\n",
    "Current Value: False (keep original formatting)\n",
    "Usage: When True, removes extra spaces/newlines\n",
    "10. input_key: 'input'\n",
    "Purpose: The key name for the input variable\n",
    "Current Value: 'input'\n",
    "Usage: When you call chain.run(\"text\"), it becomes {\"input\": \"text\"}\n",
    "11. output_key: 'output'\n",
    "Purpose: The key name for the final output\n",
    "Current Value: 'output'\n",
    "Usage: The final result is stored under this key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1fcd08",
   "metadata": {},
   "source": [
    "![image](/image/sequential_chain.png)\n",
    "\n",
    "\n",
    "![image](/image/router_chain.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
